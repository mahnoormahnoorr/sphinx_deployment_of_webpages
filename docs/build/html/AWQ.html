

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AWQ Quantization with LLM Compressor &mdash; quant v.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="_static/lumi.css?v=4a224b01" />

  
    <link rel="shortcut icon" href="_static/favicon.png"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8a49c756"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="BitsAndBytes Quantization with Transformers" href="BitsAndBytes.html" />
    <link rel="prev" title="Quantization Examples with Transformers &amp; LLM Compressor" href="GPTQ.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/LUMI_light.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="GPTQ.html">Quantization Examples with Transformers &amp; LLM Compressor</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">AWQ Quantization with LLM Compressor</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installations">Installations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#load-the-module">Load the module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-and-activate-a-virtual-environment-using-system-packages">Create and activate a virtual environment using system packages</a></li>
<li class="toctree-l3"><a class="reference internal" href="#install-packages">Install packages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#output-includes">Output Includes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#notes">Notes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="BitsAndBytes.html">BitsAndBytes Quantization with Transformers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">quant</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">AWQ Quantization with LLM Compressor</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/AWQ.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="awq-quantization-with-llm-compressor">
<h1>AWQ Quantization with LLM Compressor<a class="headerlink" href="#awq-quantization-with-llm-compressor" title="Link to this heading">ÔÉÅ</a></h1>
<p>This example script shows how to quantize a model using <strong>Activation-Aware Weight Quantization (AWQ)</strong> with the <a class="reference external" href="https://github.com/vllm-project/llm-compressor">LLM Compressor</a> toolkit. AWQ protects ~1% of the most important weight channels to reduce quantization error and improve performance when compared to uniform quantization.</p>
<p>In order to target weight and activation scaling locations within the model, the AWQModifier must be provided an AWQ mapping. The model used in the example already has these mappings provided, but in case the model you want to quantize does not, you need to add your own mappings via the mappings argument with instantiating the AWQModifier. You can check existing mappings (and contribute your own) <a class="reference external" href="https://github.com/vllm-project/llm-compressor/blob/main/src/llmcompressor/modifiers/awq/mappings.py">here.</a></p>
<section id="installations">
<h2>Installations<a class="headerlink" href="#installations" title="Link to this heading">ÔÉÅ</a></h2>
<p>The CSC preinstalled PyTorch module covers most of the libraries needed to run these examples
(torch, transformers, datasets, accelerate). The rest can be installed on top of the module in a virtual environment.</p>
<section id="load-the-module">
<h3>Load the module<a class="headerlink" href="#load-the-module" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>purge
module<span class="w"> </span>use<span class="w"> </span>/appl/local/csc/modulefiles
module<span class="w"> </span>load<span class="w"> </span>pytorch/2.7
</pre></div>
</div>
</section>
<section id="create-and-activate-a-virtual-environment-using-system-packages">
<h3>Create and activate a virtual environment using system packages<a class="headerlink" href="#create-and-activate-a-virtual-environment-using-system-packages" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>--system-site-packages<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>venv/bin/activate
</pre></div>
</div>
</section>
<section id="install-packages">
<h3>Install packages<a class="headerlink" href="#install-packages" title="Link to this heading">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>optimum<span class="w"> </span>llmcompressor<span class="w"> </span>--cache-dir<span class="w"> </span>./.pip-cache
</pre></div>
</div>
<p>The flag ‚Äìcache-dir points the pip cache to the current (scratch) folder instead of the default (home directory), to avoid filling up home directory quota.</p>
</section>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading">ÔÉÅ</a></h2>
<p>The launch scripts are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">run-awq-modifier-lumi.sh</span></code> - quantizes model on LUMI with 1 GPU</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run-awq-modifier-mahti.sh</span></code> - quantizes model on Mahti with 1 GPU</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run-awq-modifier-puhti.sh</span></code> - quantizes model on Puhti with 1 GPU</p></li>
</ul>
<p><strong>Note:</strong> the scripts are made to be run on <code class="docutils literal notranslate"><span class="pre">gputest</span></code> or <code class="docutils literal notranslate"><span class="pre">dev-g</span></code> partition with a 30 minutes time-limit. You have to select the proper partition for longer jobs for your real runs. Additionally, change the <code class="docutils literal notranslate"><span class="pre">--account</span></code> parameter to your own project code.</p>
<p>For example to run on LUMI, you would run the command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch<span class="w"> </span>run-awq-modifier-lumi.sh
</pre></div>
</div>
<p>You can also increase the memory and number of GPUs if you decide to run quantization on larger models.</p>
<p>The script will quantize the <strong>Falcon-RW-1B</strong> model using the following recipe:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">recipe</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>AWQModifier<span class="o">(</span><span class="nv">targets</span><span class="o">=</span><span class="s2">&quot;Linear&quot;</span>,<span class="w"> </span><span class="nv">scheme</span><span class="o">=</span><span class="s2">&quot;W4A16&quot;</span>,<span class="w"> </span><span class="nv">ignore</span><span class="o">=[</span><span class="s2">&quot;lm_head&quot;</span><span class="o">])</span>
</pre></div>
</div>
<p>Meaning the script quantizes the model‚Äôs linear layers using a mixed-precision approach: weights are reduced to 4-bit while activations remain at 16-bit to retain higher accuracy. The lm_head layer (the model‚Äôs output projection) is excluded to preserve output quality. You can check the output in the output file.</p>
</section>
<section id="output-includes">
<h2>Output Includes<a class="headerlink" href="#output-includes" title="Link to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Generated text before and after quantization.</p></li>
<li><p>Inference time comparison. Note that the effect of quantization on inference might not be noticeable for smaller models.</p></li>
<li><p>Model size (MB) before and after quantization.</p></li>
</ul>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Link to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>The current scripts use <strong>Falcon-RW-1B</strong> for fast experimentation. You can replace <code class="docutils literal notranslate"><span class="pre">model_name</span></code> with a larger model. In this case, you might want to disable saving the full model.</p></li>
<li><p>For large models, <code class="docutils literal notranslate"><span class="pre">device_map=&quot;auto&quot;</span></code> lets ü§ó Accelerate handle placement across GPUs.</p></li>
<li><p>Feel free to experiment with different values for <code class="docutils literal notranslate"><span class="pre">num_calibration_samples</span></code> and <code class="docutils literal notranslate"><span class="pre">max_seq_lenght</span></code> and to modify the quantization recipe.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="GPTQ.html" class="btn btn-neutral float-left" title="Quantization Examples with Transformers &amp; LLM Compressor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="BitsAndBytes.html" class="btn btn-neutral float-right" title="BitsAndBytes Quantization with Transformers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, Mahnoor Mahnoor.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>